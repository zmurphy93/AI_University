{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - An Overview\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Logistic Regression Classifier\n",
    "<br><br>\n",
    "\n",
    "We'll use the Pima Indians Diabetes dataset to aid in our construction of this classifier. The dataset describes whether an individual's health data, as well as whether or not they developed diabetes within 5 years of the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data=pd.read_csv(\"C:/Users/zmurp/github/Machine_Learning_Algorithms/datasets/diabetes.csv\")\n",
    "data.median()\n",
    "dataset = data.fillna(data.median())\n",
    "X = dataset.iloc[:500, 0:8].values\n",
    "ytrain = dataset.iloc[:500, 8].values\n",
    "scaler = StandardScaler()\n",
    "Xtrain = scaler.fit_transform(X)\n",
    "\n",
    "Xtest=scaler.fit_transform(dataset.iloc[500:, 0:8].values)\n",
    "ytest=dataset.iloc[500:, 8].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data now loaded in, we can get started on constructing the classifier. We'll create a class called <code>LogisticRegression()</code> and add in some attributes that will be suitable for this type of algorithm. Some of these were mentioned above, while others will be explained in more depth later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import types\n",
    "import math\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, C=5.0, iterations=6000, initialization_scheme=\"zeroes\", use_intercept=False, learning_rate=0.1, penalization_type='l2'):\n",
    "        self.C = C\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initialization_scheme=initialization_scheme\n",
    "        self.use_intercept=use_intercept\n",
    "        self.learning_rate=learning_rate\n",
    "        self.penalization_type = penalization_type\n",
    "        \n",
    "logistic_regression = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes that we have defined for our logistic regression class are as follows:\n",
    "        <br><br>\n",
    "    <strong>C</strong>: Regularization parameter. Increasing values are associated with stronger regularization of the algorithm. \n",
    "        <br><br>\n",
    "    <strong>iterations</strong>: The number of iterations that the training process will go through before completing.\n",
    "        <br><br>\n",
    "    <strong>use_intercept</strong>: When true, this adds an extra column of ones to the training dataset. When false, the original unaltered training dataset is used.\n",
    "        <br><br>\n",
    "    <strong>learning_rate</strong>: A small constant that is applied to the optimization algorithm in order to improve convergence.\n",
    "        <br><br>\n",
    "    <strong>penalization_type</strong>: Whether to use L1-penalization, L2-penalization, or no penalizations in training the logistic regression algorithm.\n",
    "        <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _include_intercept(self, X):\n",
    "    if self.use_intercept=False\n",
    "        return X\n",
    "    if self.use_intercept==True:\n",
    "        X = np.c_[np.ones([np.shape(X)[0], 1]), X]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first order of business is to initialize the weights that will ultimately be optimized throughout the training process. There are a number of ways to initialize the weights, but the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def _initialize_weights(self, X, initialization_scheme):\n",
    "    Xtrain = self.include_intercept(X)\n",
    "    if self.initialization_scheme==\"zeroes\":\n",
    "        self.weights = np.zeros(np.shape(Xtrain)[1]) \n",
    "    elif self.initialization_scheme==\"random\":\n",
    "        self.weights = np.random.uniform(-1 * 0.9, 0.9, np.shape(Xtrain)[1])\n",
    "    elif self.initialization_scheme==\"he\":\n",
    "        n_features = np.shape(Xtrain)[1]\n",
    "        bound = 1 / math.sqrt(n_features)\n",
    "        self.weights = np.random.uniform(-1 * bound, bound, (n_features))\n",
    "    else:\n",
    "        print(\"Invalid initialization. Please select from 'zeroes', 'random', or'he'.\")\n",
    "\n",
    "        \n",
    "logistic_regression._initialize_weights = types.MethodType(_initialize_weights, logistic_regression)\n",
    "\n",
    "logistic_regression._initialize_weights(Xtrain=Xtrain, initialization_scheme=\"he\")\n",
    "print(logistic_regression.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we must do is to modify the function above into a method that can apply the sigmoid function to our training data. In doing this, the weights that we generated above are also multiplied with the features in the training dataset via a dot product. The results of this dot product are returned as a list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(self, X):\n",
    "        z = np.dot(X, self.weights)\n",
    "        s = 1.0 / (1 + np.exp(-z))\n",
    "        return s   \n",
    "logistic_regression._sigmoid = types.MethodType(_sigmoid, logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third basic function that we will need will be a function that describes the cost function. A cost function is a mathematical relationship that measures the ability of a machine learning algorithm to determine the relationship between features and their labels in a dataset. A variety of cost functions exist and have specific use-cases for certain algorithms. The cost function that will be used here will be the log-likelihood cost function, which is shown below.\n",
    "<br><br>\n",
    "For some algorithms, cost functions have the ability to be <em>regularized.</em> That means that their values can be influenced by the addition of an extra parameter. The existence of this parameter dictates how much wrong classifications should be penalized during the training process. The most common types of regularizations implemented for a logistic regression algorithm are L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.321698784996585\n"
     ]
    }
   ],
   "source": [
    "def _cost_function(self, X, y, penalization_type):\n",
    "    m = Xtrain.shape[1]\n",
    "    if penalization_type== None:\n",
    "        cost = -1/m * (np.sum((y * np.log(self._sigmoid(X))) + ((1 - y) * np.log(1 - self._sigmoid(X)))))\n",
    "    elif penalization_type=='l1':\n",
    "        cost = -1/m * (np.sum((y * np.log(self._sigmoid(X))) + ((1 - y) * np.log(1 - self._sigmoid(X))))) + (1 / (m * self.C)) * np.sum(np.abs(self.weights))\n",
    "    elif penalization_type=='l2':\n",
    "        cost = -1/m * (np.sum((y * np.log(self._sigmoid(X))) + ((1 - y) * np.log(1 - self._sigmoid(X))))) + (1 / (2 *m* self.C)) * np.dot(self.weights.T, self.weights)\n",
    "    else:\n",
    "        print(\"Invalid penalization_type. Please enter 'None', 'l1', or 'l2'.\")\n",
    "    return cost\n",
    "\n",
    "logistic_regression._cost_function = types.MethodType(_cost_function, logistic_regression)\n",
    "print(logistic_regression._cost_function(Xtrain=Xtrain, ytrain=ytrain, penalization_type='l1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these two methods, we can write our training method. The training method will accomplish the following tasks.\n",
    "<br><br>\n",
    "1.) It will initialize our weights using the <code>initialize_weights</code> from above.\n",
    "<br><br>\n",
    "2.) Our training data will be multiplied by these weights and then transformed by the sigmoid function to make a prediction.\n",
    "<br><br>\n",
    "3.) The differences between the sigmoid functions values and the training labels are computed to measure the overall error.\n",
    "\n",
    "4.) An optimization algorithm called <em>gradient descent</em> will then iteratively adjust the weights based on the resulting value of the cost function in an effort to minimize it on the next iteration. Note that there are slightly different formulations of this depending on what penalization scheme is used.\n",
    "<br><br>\n",
    "Steps 1 - 4 then repeat until all iterations are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.31055321 6.69281647 0.56905497 0.80892749 2.07386126 4.64155386\n",
      " 2.67219163 3.10528757]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zmurp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "C:\\Users\\zmurp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def train(self, X, y):\n",
    "    m = Xtrain.shape[1]\n",
    "    self._initialize_weights(Xtrain, self.initialization_scheme)\n",
    "    self.costs = []\n",
    "    for i in range(self.iterations):\n",
    "            s = self._sigmoid(Xtrain)\n",
    "            errors = ytrain - s\n",
    "            if self.penalization_type==None:\n",
    "                delta_w = self.learning_rate * (1/m)*np.dot(errors, Xtrain)\n",
    "            elif self.penalization_type==\"l1\":\n",
    "                delta_w = self.learning_rate * (self.C * ((1/m)*np.dot(errors, Xtrain)) + np.sum(np.sign(self.weights))) \n",
    "            elif self.penalization_type==\"l2\":                \n",
    "                delta_w = self.learning_rate * (self.C * ((1/m)*np.dot(errors, Xtrain)) + np.sum(self.weights))  \n",
    "            self.iterationsPerformed = i\n",
    "    self.weights += delta_w                                \n",
    "                #Costs\n",
    "    self.costs.append(self._cost_function(Xtrain, ytrain, self.penalization_type)) \n",
    "    return self                \n",
    "     \n",
    "\n",
    "logistic_regression.train = types.MethodType(train, logistic_regression)\n",
    "logistic_regression.train(Xtrain, ytrain)\n",
    "print(logistic_regression.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our trained weights, we can now make predictions with them. All we have to do is apply the sigmoid function (with the adjusted weights in them) to our test set to get a sense on how it does on unseen data. Recall that the logistic regression algorithm is a probabilistic classifier, and that its outputs are probabilities that it belongs to the positive class. We can apply a quick transformation to the probabilities to come up with binary classification predictions. We say that a positive value is one that has a probability > 0.5, and all others are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, Xtest):\n",
    "    predictions=self._sigmoid(Xtest)\n",
    "    det_pred = []\n",
    "    for pred in predictions:\n",
    "        if pred>0.50:\n",
    "            p = 1\n",
    "            det_pred.append(p)\n",
    "        else:\n",
    "            p=0\n",
    "            det_pred.append(0)\n",
    "    return det_pred\n",
    "    \n",
    "logistic_regression.predict = types.MethodType(predict, logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performanceEval(predictions, y_test):\n",
    "       \n",
    "        #Initialize\n",
    "        TP, TN, FP, FN, P, N = 0, 0, 0, 0, 0, 0\n",
    "        \n",
    "        for idx, test_sample in enumerate(y_test):\n",
    "            \n",
    "            if predictions[idx] == 1 and test_sample == 1:\n",
    "                TP += 1       \n",
    "                P += 1\n",
    "            elif predictions[idx] == 0 and test_sample == 0:                \n",
    "                TN += 1\n",
    "                N += 1\n",
    "            elif predictions[idx] == 0 and test_sample == 1:\n",
    "                FN += 1\n",
    "                P += 1\n",
    "            elif predictions[idx] == 1 and test_sample == 0:\n",
    "                FP += 1\n",
    "                N += 1\n",
    "            \n",
    "        accuracy = (TP + TN) / (P + N)                \n",
    "        sensitivity = TP / P        \n",
    "        specificity = TN / N        \n",
    "        PPV = TP / (TP + FP)        \n",
    "        NPV = TN / (TN + FN)        \n",
    "        FNR = 1 - sensitivity        \n",
    "        FPR = 1 - specificity\n",
    "        \n",
    "        performance = {'Accuracy': accuracy, 'Sensitivity': sensitivity,\n",
    "                       'Specificity': specificity, 'Precision': PPV,\n",
    "                       'NPV': NPV, 'FNR': FNR, 'FPR': FPR}   \n",
    "        \n",
    "        conf_matrix1 = {'TP': TP, 'FN': FN}\n",
    "        conf_matrix2 = {'FP': FP, 'TN': TN}\n",
    "        \n",
    "        return conf_matrix1, conf_matrix2, performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our complete class is below. This will also be made available in the following Github repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import types\n",
    "import math\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, C=5.0, iterations=6000, initialization_scheme=\"zeroes\", learning_rate=0.1, penalization_type='l2'):\n",
    "        self.C = C\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initialization_scheme=initialization_scheme\n",
    "        self.penalization_type = penalization_type\n",
    "    \n",
    "    def _initialize_weights(self, Xtrain, initialization_scheme):\n",
    "        if self.initialization_scheme==\"zeroes\":\n",
    "            self.weights = np.zeros(np.shape(Xtrain)[1]) \n",
    "        elif self.initialization_scheme==\"random\":\n",
    "            self.weights = np.random.uniform(-1 * 0.9, 0.9, (np.shape(Xtrain)[1]))\n",
    "        elif self.initialization_scheme==\"he\":\n",
    "            n_features = np.shape(Xtrain)[1]\n",
    "            bound = 1 / math.sqrt(n_features)\n",
    "            self.weights = np.random.uniform(-1 * bound, bound, (n_features))\n",
    "        else:\n",
    "            print(\"Invalid initialization. Please select from 'zeroes', 'random', or'he'.\")\n",
    "            \n",
    "    def _sigmoid(self, Xtrain):\n",
    "        z = np.dot(Xtrain, self.weights)\n",
    "        s = 1.0 / (1 + np.exp(-z))\n",
    "        return s  \n",
    "    \n",
    "    def _cost_function(self, Xtrain, ytrain, penalization_type):\n",
    "        m = Xtrain.shape[1]\n",
    "        if penalization_type== None:\n",
    "            cost = -1/m * (np.sum((ytrain * np.log(self._sigmoid(Xtrain))) + ((1 - ytrain) * np.log(1 - self._sigmoid(Xtrain)))))\n",
    "        elif penalization_type=='l1':\n",
    "            cost = -1/m * (np.sum((ytrain * np.log(self._sigmoid(Xtrain))) + ((1 - ytrain) * np.log(1 - self._sigmoid(Xtrain))))) + (1 / (m * self.C)) * np.sum(np.abs(self.weights))\n",
    "        elif penalization_type=='l2':\n",
    "            cost = -1/m * (np.sum((ytrain * np.log(self._sigmoid(Xtrain))) + ((1 - ytrain) * np.log(1 - self._sigmoid(Xtrain))))) + (1 / (2 *m* self.C)) * np.dot(self.weights.T, self.weights)\n",
    "        else:\n",
    "            print(\"Invalid penalization_type. Please enter 'None', 'l1', or 'l2'.\")\n",
    "        return cost\n",
    "    \n",
    "    def train(self, Xtrain, ytrain):\n",
    "        m = Xtrain.shape[1]\n",
    "        self._initialize_weights(Xtrain, self.initialization_scheme)\n",
    "        self.costs = []\n",
    "        for i in range(self.iterations):\n",
    "                s = self._sigmoid(Xtrain)\n",
    "                errors = ytrain - s\n",
    "                if self.penalization_type==None:\n",
    "                    delta_w = self.learning_rate * (1/m)*np.dot(errors, Xtrain)\n",
    "                elif self.penalization_type==\"l1\":\n",
    "                    delta_w = self.learning_rate * (self.C * ((1/m)*np.dot(errors, Xtrain)) + np.sum(np.sign(self.weights))) \n",
    "                elif self.penalization_type==\"l2\":                \n",
    "                    delta_w = self.learning_rate * (self.C * ((1/m)*np.dot(errors, Xtrain)) + np.sum(self.weights))  \n",
    "                self.iterationsPerformed = i\n",
    "        self.weights += delta_w                                \n",
    "                #Costs\n",
    "        self.costs.append(self._cost_function(Xtrain, ytrain, self.penalization_type)) \n",
    "        return self        \n",
    "    \n",
    "    def predict(self, Xtest):\n",
    "        predictions=self._sigmoid(Xtest)\n",
    "        det_pred = []\n",
    "        for pred in predictions:\n",
    "            if pred>0.50:\n",
    "                p = 1\n",
    "                det_pred.append(p)\n",
    "            else:\n",
    "                p=0\n",
    "                det_pred.append(0)\n",
    "        return det_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 70, 'FN': 16}\n",
      "{'FP': 51, 'TN': 131}\n",
      "{'Accuracy': 0.75, 'Sensitivity': 0.813953488372093, 'Specificity': 0.7197802197802198, 'Precision': 0.5785123966942148, 'NPV': 0.891156462585034, 'FNR': 0.18604651162790697, 'FPR': 0.2802197802197802}\n",
      "{'TP': 71, 'FN': 15}\n",
      "{'FP': 49, 'TN': 133}\n",
      "{'Accuracy': 0.7611940298507462, 'Sensitivity': 0.8255813953488372, 'Specificity': 0.7307692307692307, 'Precision': 0.5916666666666667, 'NPV': 0.8986486486486487, 'FNR': 0.17441860465116277, 'FPR': 0.2692307692307693}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zmurp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\zmurp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\zmurp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, iterations=30000, initialization_scheme=\"zeroes\", learning_rate=1, penalization_type='l1')\n",
    "clf.train(Xtrain, ytrain)\n",
    "cm1, cm2, per = performanceEval(clf.predict(Xtest), ytest)\n",
    "print(cm1)\n",
    "print(cm2)\n",
    "print(per)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "clf2=lr(penalty='l1',C=10.0, fit_intercept=False, solver='saga', max_iter=30000)\n",
    "clf2.fit(Xtrain, ytrain)\n",
    "skpreds = clf2.predict(Xtest)\n",
    "skcm1, skcm2, skper = performanceEval(skpreds, ytest)\n",
    "\n",
    "print(skcm1)\n",
    "print(skcm2)\n",
    "print(skper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

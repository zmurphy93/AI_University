"""
Decision Tree
"""

# Author: Zachary B. Murphy


import numpy as np
import types
import math
class DecisionNode():
    """Decision Node

    A node to be used in building the decision tree.

     Parameters
    ----------
    feature_index : index of the feature that is being tested.

    threshold : the threshold value for a feature, used to make a split in the
    data.

    leaf_value: Value the appears if the node becomes a leaf in the tree.

    true_branch : :left" side of the split, used to denote when a condition is
    true.

    false_branch : "right" side of the split, used to denote when a condition
    is false.

    """


    def __init__(self, feature_i=None, threshold=None,
                 value=None, true_branch=None, false_branch=None):
        self.feature_index = feature_index          # Index for the feature that is tested
        self.threshold = threshold          # Threshold value for feature
        self.leaf_value = leaf_value                  # Value if the node is a leaf in the tree
        self.true_branch = true_branch      # 'Left' subtree
        self.false_branch = false_branch    # 'Right' subtree



class DecisionTreeClassifier(object):
    """Decision Tree Classification

    A binary classifier with a linear decision boundary,generated by fitting a
    sigmoid function to a training dataset and iteratively optimizing a set of
    weights via gradient descent.


    Parameters
    ----------
    k : int, optional (default=5)
        Number of neighbors used in the classification process.

    similarity_metric : string, optional (default="euclidean")
       The metric used to evaluate distances during the classification process.
       Possible values:

           - "euclidean":

           - "manhattan":

           - "minkowski":

           - "chebyshev":

    weights: string, optional (default="uniform")
        Weights to assign to the distances of the k-nearest neighbors during
        the classification process. Possible values:
            -"uniform": Uniform weights. All points are weighted equally.

            -"distance": weight distances of each neighbor by the inverse of
            their distance to the point in question. In this case, closer
            neighbors of a query point will have a greater influence than
            those which are further away.

    p: float, optional (default=5.0)
    Parameter for the Minkowski similarity metric. When p = 1, this is
    equivalent to using "manhattan" similarity, and "euclidean" similarity
    for p = 2.

        """
    def __init__(self, impurity_metric= 'gini', min_samples_split=2, min_impurity=1e-7, max_depth=float("inf"), loss=None):
        self.root = None
        self.impurity_metric = impurity_metric
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.min_impurity = min_impurity
        self.max_depth = max_depth
        self.max_features = max_features
        self.min_impurity_split = min_impurity_split
        self.max_leaf_nodes=max_leaf_nodes
        self._leaf_value_calculation = None

    def _entropy(self, y):
        unique_labels = np.unique(y)
        entropy = 0
        for label in unique_labels:
            count = len(y[y == label])
            p = count / len(y)
            entropy += -p * (math.log(p)/math.log(2))
        return entropy


    def _gini(self, y):
        unique_labels = np.unique(y)
        gini = 0
        for label in unique_labels:
            count = len(y[y == label])
            p = count / len(y)
            gini += -p * (1-p)
        return gini


    def _information_gain(self, self.impurity_metric):
        p = len(y1) / len(y)
        if self.impurity_metric == "entropy":
            entropy = self._entropy(y)
            info_gain = entropy - p * self._entropy(y1) - (1 - p) * self._entropy(y2)
        else if self.impurity_metric == "gini":
            gini = self._gini(y)
            info_gain = gini - p * self._gini(y1) - (1 - p) * self._gini(y2)
        return info_gain    





if __name__ == "__main__":
    import pandas as pd
    from sklearn.preprocessing import StandardScaler

    data=pd.read_csv("C:/Users/zmurp/github/Machine_Learning_Algorithms/datasets/diabetes.csv")
    data.median()
    dataset = data.fillna(data.median())
    X = dataset.iloc[:500, 0:8].values
    ytrain = dataset.iloc[:500, 8].values
    scaler = StandardScaler()
    Xtrain = scaler.fit_transform(X)

    Xtest=scaler.fit_transform(dataset.iloc[500:, 0:8].values)
    ytest=dataset.iloc[500:, 8].values

    covar = KNearestNeighborsClassifier().compute_class_covariance(Xtrain, ytrain)
    print(covar)

"""
Logistic Regression
"""

# Author: Zachary B. Murphy


import numpy as np
import math

class LogisticRegressionClassifier():
    """Logistic Regression
    
    A binary classifier with a linear decision boundary,generated by fitting a
    sigmoid function to a training dataset and iteratively optimizing a set of 
    weights via gradient descent.
    
    
    Parameters
    ----------
    C : float, optional (default=2.0)
        Penalization to be used in L1 or L2 penalization.
          
    iterations : int, optional (default=5000)
        Number of iterations that the optimization process must go through 
        before training is finished.
        
    initialization_scheme : string, optional (default='zeroes')
        Describes how to initalize the weights, possible values:
          - 'zeroes': Weights are set to zero.
          
          - 'random': Weights are randomly generated from a distribution
          between -1 and 1.
          
          - 'he': Uses the Xavier-He initialization scheme, commonly used in 
          neural network architectures.
        
    learning_rate : float, optional (default=0.1)
        A constant that is used to allow the gradient descent algorithm to 
        converge to a minimum.
        
    penalization_type : string, optional (default='l2')
         Chooses what penalization to use, if any. Possible values:
          - 'None': No penalization is implemented.
          
          - 'l1': Uses L1-penalization
          
          - 'l2': Uses L2-penalization
        
    use_intercept : bool, optional, (default = False)
        When set to "True", adds an additional weight to the initialized
        weights for the algorithm to optimize. When set to false, the weights
        array is set to np.shape(Xtrain)[1] where Xtrain is a set of features 
        from a training dataset.
        
        """
    def __init__(self, C=2.0, iterations=6000, initialization_scheme="zeroes", learning_rate=0.1, penalization_type='l2', use_intercept=False):
        self.C = C
        self.iterations = iterations
        self.learning_rate = learning_rate
        self.initialization_scheme=initialization_scheme
        self.penalization_type = penalization_type
        self.use_intercept= use_intercept
    
    def _initialize_weights(self, Xtrain, initialization_scheme, use_intercept):
        """ Weight initialization.
        
        Initializes the weights for the logistic regression to optimize during
        training. Note an "intercept" weight is used if "use_intercept" is 
        set to be "True".
        
        Parameters
        ----------
        Xtrain : array-like, shape (n_samples, n_features)
            Training data.
       
        
        initialization_scheme : string
        Describes how to initalize the weights, possible values:
            
        - 'zeroes': Weights are set to zero.
          
        - 'random': Weights are randomly generated from a distribution
          between -1 and 1.
          
        - 'he': Uses the Xavier-He initialization scheme, commonly used in 
          neural network architectures.
          
        use_intercept : bool
        When set to "True", adds an additional weight to the initialized
        weights for the algorithm to optimize. When set to false, the weights
        array is set to np.shape(Xtrain)[1] where Xtrain is a set of features 
        from a training dataset.
        """
        if self.use_intercept==True:
            Xtrain = np.c_[np.ones([np.shape(Xtrain)[0], 1]), Xtrain]
        else:
            Xtrain = Xtrain
        
        if self.initialization_scheme=="zeroes":
            self.weights = np.zeros(np.shape(Xtrain)[1]) 
        elif self.initialization_scheme=="random":
            self.weights = np.random.uniform(-1 * 0.9, 0.9, (np.shape(Xtrain)[1]))
        elif self.initialization_scheme=="he":
            n_features = np.shape(Xtrain)[1]
            bound = 1 / math.sqrt(n_features)
            self.weights = np.random.uniform(-1 * bound, bound, (n_features))
        else:
            print("Invalid initialization. Please select from 'zeroes', 'random', or'he'.")
            
    def _sigmoid(self, Xtrain):
        """ Sigmoid transformation.
        
        Fit a sigmoid function to a set of data and associated weights, where
        the sigmoid function is defined by
        S(x) = 1/(1+exp(-z))
        
        Parameters
        ----------
        Xtrain : array-like, shape (n_samples, n_features)
            Training data.
       """
       
        z = np.dot(Xtrain, self.weights)
        s = 1.0 / (1 + np.exp(-z))
        return s 
        
    
    def _cost_function(self, Xtrain, ytrain, penalization_type):
        """ Cost function.
        
        Uses the log loss cost function to evaluate the predictions generated
        on each iteration of the training process. The cost function is
        modified according to what kind of penalization is specified upon
        initialization. The value of C that is also specified during
        initialization is also implemented in the following formulations.
        
        Parameters
        ----------
        Xtrain : array-like, shape (n_samples, n_features)
            Training data.
        ytrain : array, shape (n_samples,)
            Target values.
    
        
        penalization_type : string, optional (default='l2')
         Chooses what penalization to use, if any. Possible values:
          - 'None': No penalization is implemented.
          
          - 'l1': Uses L1-penalization
          
          - 'l2': Uses L2-penalization
        
        
        Returns
        ----------
        cost: the computed cost for the current step.
        
        """
        
        m = Xtrain.shape[1]
        if penalization_type== None:
            cost = -1/m * (np.sum((ytrain * np.log(self._sigmoid(Xtrain))) + ((1 - ytrain) * np.log(1 - self._sigmoid(Xtrain)))))
        elif penalization_type=='l1':
            cost = -1/m * (np.sum((ytrain * np.log(self._sigmoid(Xtrain))) + ((1 - ytrain) * np.log(1 - self._sigmoid(Xtrain))))) + (1 / (m * self.C)) * np.sum(np.abs(self.weights))
        elif penalization_type=='l2':
            cost = -1/m * (np.sum((ytrain * np.log(self._sigmoid(Xtrain))) + ((1 - ytrain) * np.log(1 - self._sigmoid(Xtrain))))) + (1 / (2 *m* self.C)) * np.dot(self.weights.T, self.weights)
        else:
            print("Invalid penalization_type. Please enter 'None', 'l1', or 'l2'.")
        return cost
    
    def train(self, Xtrain, ytrain):
        
        """Fit Logistic Regression model according to the given training data 
        and parameters. Weights are optimized using a gradient descent
        algorithm.
           
         Parameters
         ----------
        
         Xtrain : array-like, shape (n_samples, n_features)
            Training data.
         ytrain : array, shape (n_samples,)
            Target values.
         """
        m = Xtrain.shape[1]
        self._initialize_weights(Xtrain, self.initialization_scheme)
        self.costs = []
        for i in range(self.iterations):
                s = self._sigmoid(Xtrain)
                errors = ytrain - s
                if self.penalization_type==None:
                    delta_w = self.learning_rate * (1/m)*np.dot(errors, Xtrain)
                elif self.penalization_type=="l1":
                    delta_w = self.learning_rate * (self.C * ((1/m)*np.dot(errors, Xtrain)) + np.sum(np.sign(self.weights))) 
                elif self.penalization_type=="l2":                
                    delta_w = self.learning_rate * (self.C * ((1/m)*np.dot(errors, Xtrain)) + np.sum(self.weights))  
                self.iterationsPerformed = i
        self.weights += delta_w                                
                #Costs
        self.costs.append(self._cost_function(Xtrain, ytrain, self.penalization_type)) 
        return self        
    
    def predict(self, Xtest):
        """Perform classification on an array of test vectors Xtest.
        The predicted class det_pred for each sample in X is returned.
        
        Parameters
        ----------
        Xtest : array-like, shape = [n_samples, n_features]
        
        Returns
        -------
        det_pred : array, shape = [n_samples]
        """
        if self.use_intercept==True:
            predictions = 1.0 / (1 + np.exp(-1*(self.weights[0]+np.dot(Xtest, self.weights[1:]))))
        else:
            predictions=self._sigmoid(Xtest)
        det_pred = []
        for pred in predictions:
            if pred>0.50:
                p = 1
                det_pred.append(p)
            else:
                p=0
                det_pred.append(0)
        return det_pred
    